# Neural-Networks-from-Scratch
Developed a neural network from scratch in Python to classify the spiral data. The project involved implementing core neural network components, including:

- **Dense Layers:** Built a class for dense (fully connected) layers, handling weight and bias initialization and the forward and backward passes.
- **Activation Functions:** Implemented ReLU and Softmax activation functions, including their respective backward passes.
- **Loss Function:** Created a Cross-Entropy loss function and combined it with the Softmax activation for efficient backpropagation.
- **Optimizer:** Developed an Adam optimizer to update weights and biases based on gradients calculated during the backward pass, incorporating momentum and adaptive learning rates.
- **Training Loop:** Constructed a training loop to iterate through epochs, perform forward and backward passes, calculate loss and accuracy, and update model parameters using the Adam optimizer.
- **Decision Boundary Visualization:** Implemented a function to visualize the decision boundary learned by the trained model on the spiral dataset, demonstrating the network's ability to separate the classes.

This project showcases a strong understanding of the fundamental building blocks of neural networks and the backpropagation algorithm for training.
